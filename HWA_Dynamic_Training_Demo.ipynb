{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sg8v1SCn6HSe",
        "outputId": "b0743e2a-cef2-4d3c-aaec-980480fc40f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aihwkit\n",
            "  Downloading aihwkit-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting torch==2.4.1 (from aihwkit)\n",
            "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from aihwkit) (0.20.1+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from aihwkit) (1.13.1)\n",
            "Requirement already satisfied: requests<3,>=2.25 in /usr/local/lib/python3.10/dist-packages (from aihwkit) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from aihwkit) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=4.21.6 in /usr/local/lib/python3.10/dist-packages (from aihwkit) (4.25.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->aihwkit) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->aihwkit) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->aihwkit) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->aihwkit) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->aihwkit) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->aihwkit) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1->aihwkit)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.1->aihwkit)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->aihwkit) (12.6.85)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (2024.12.14)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from aihwkit)\n",
            "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->aihwkit) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->aihwkit) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->aihwkit) (1.3.0)\n",
            "Downloading aihwkit-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, aihwkit\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aihwkit-0.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 torchvision-0.19.1 triton-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install aihwkit\n",
        "!wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-0.9.1+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "!pip install aihwkit-0.9.1+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MTL Model Adapted from: https://medium.com/@aminul.huq11/multi-task-learning-a-beginners-guide-a1fc17808688"
      ],
      "metadata": {
        "id": "sSvy6RsKPHUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aminul-huq/medium.git"
      ],
      "metadata": {
        "id": "TNAf_VMxNqYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd medium/mammogram\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader,random_split,Dataset\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from training_utils import *"
      ],
      "metadata": {
        "id": "iXWwVizEN1oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 43\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "D-tw00Y7N8PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.CIFAR10(root='./data/', train=True, download=True, transform=transforms.ToTensor())\n",
        "testset = datasets.CIFAR10(root='./data/', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "labels_list = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "non_animal = [0,1,8,9]\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "inAacJesN9CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewDataset(Dataset):\n",
        "\n",
        "    def __init__(self,data,transform=None):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        image = self.data[idx][0]\n",
        "        label1 = self.data[idx][1]                                  #original label\n",
        "        label2 = 0 if self.data[idx][1] in non_animal else 1        #animal or non-animal\n",
        "        return image, label1, label2"
      ],
      "metadata": {
        "id": "H0UEJZhVOHAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_trainset = NewDataset(trainset,non_animal)\n",
        "new_testset = NewDataset(testset,non_animal)\n",
        "\n",
        "train_set, valid_set = random_split(new_trainset,[int(len(new_trainset)*0.9), int(len(new_trainset)*0.1)],\n",
        "                                  generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(new_testset, batch_size=100, shuffle=True)"
      ],
      "metadata": {
        "id": "_zP8MAZ2OIkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aihwkit.simulator.configs import InferenceRPUConfig\n",
        "from aihwkit.nn import AnalogLinear\n",
        "from aihwkit.optim import AnalogSGD\n",
        "from aihwkit.simulator.configs import (\n",
        "    InferenceRPUConfig,\n",
        "    WeightNoiseType,\n",
        "    WeightClipType,\n",
        "    WeightModifierType,\n",
        "    WeightRemapType,\n",
        ")\n",
        "from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n",
        "from aihwkit.inference import PCMLikeNoiseModel, GlobalDriftCompensation\n",
        "from aihwkit.simulator.parameters import IOParameters"
      ],
      "metadata": {
        "id": "5y0XMlgOM9_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rpu_config(g_max=25, tile_size=512, modifier_std=0.07):\n",
        "  rpu_config = InferenceRPUConfig()\n",
        "  rpu_config.mapping.digital_bias = True\n",
        "  rpu_config.mapping.weight_scaling_omega = 1.0\n",
        "  rpu_config.mapping.weight_scaling_columnwise = True\n",
        "  rpu_config.mapping.learn_out_scaling = True\n",
        "  rpu_config.mapping.out_scaling_columnwise = True\n",
        "  rpu_config.mapping.max_input_size = tile_size\n",
        "  rpu_config.mapping.max_output_size = tile_size\n",
        "  rpu_config.noise_model = PCMLikeNoiseModel(g_max=g_max)\n",
        "  rpu_config.remap.type = WeightRemapType.CHANNELWISE_SYMMETRIC\n",
        "  rpu_config.clip.type = WeightClipType.FIXED_VALUE\n",
        "  rpu_config.clip.fixed_value = 1.0\n",
        "  rpu_config.modifier.type = WeightModifierType.MULT_NORMAL\n",
        "  rpu_config.modifier.rel_to_actual_wmax = True\n",
        "  rpu_config.modifier.std_dev = modifier_std\n",
        "  rpu_config.forward = IOParameters()\n",
        "  rpu_config.forward.out_noise = 0.05\n",
        "  rpu_config.forward.inp_res = 1 / (2 ** 8 - 2)\n",
        "  rpu_config.forward.out_res = 1 / (2 ** 8 - 2)\n",
        "  rpu_config.drift_compensation = GlobalDriftCompensation()\n",
        "  return rpu_config"
      ],
      "metadata": {
        "id": "4wDcWgI08G44"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hardware_metrics(model, outputs):\n",
        "  rpu_config = model.rpu_config\n",
        "  temperature = np.random.uniform(0.8, 1.2) * rpu_config.forward.inp_res * rpu_config.forward.out_res\n",
        "  noise = np.random.normal(0, rpu_config.forward.out_noise)\n",
        "  drift = rpu_config.drift_compensation.readout(outputs)\n",
        "  return temperature, noise, drift"
      ],
      "metadata": {
        "id": "6j480FAM6_W4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hardware_loss(model, outputs):\n",
        "  temperature, noise, drift = compute_hardware_metrics(model, outputs)\n",
        "  hardware_loss = 0.2 * temperature + 0.3 * noise + 0.2 * drift\n",
        "  return hardware_loss"
      ],
      "metadata": {
        "id": "HsxQbJbACDqG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_rpu_config(rpu_config, perturbation_scale=0.01):\n",
        "  rpu_config.forward.inp_res += rpu_config.forward.inp_res * perturbation_scale\n",
        "  rpu_config.forward.out_res += rpu_config.forward.inp_res * perturbation_scale\n",
        "  rpu_config.forward.out_noise += rpu_config.forward.inp_res * perturbation_scale\n",
        "  g_max = rpu_config.noise_model.g_max + rpu_config.noise_model.g_max * perturbation_scale\n",
        "  rpu_config.noise_model = PCMLikeNoiseModel(g_max)\n",
        "  rpu_config.modifier.std_dev += modifier_std * perturbation_scale"
      ],
      "metadata": {
        "id": "2MZdbvOsE3xC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicLayer(nn.Module):\n",
        "    def __init__(self, layer_type, *args, **kwargs):\n",
        "\n",
        "        super(DynamicLayer, self).__init__()\n",
        "        self.layer_type = layer_type\n",
        "        self.args = args\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "        rpu_config = create_rpu_config()\n",
        "        if layer_type == 'conv':\n",
        "            self.analog_layer = AnalogConv2d(*args, **kwargs, rpu_config=rpu_config)\n",
        "            self.digital_layer = nn.Conv2d(*args, **kwargs)\n",
        "        elif layer_type == 'linear':\n",
        "            self.analog_layer = AnalogLinear(*args, **kwargs, rpu_config=rpu_config)\n",
        "            self.digital_layer = nn.Linear(*args, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid layer_type. Use 'conv' or 'linear'.\")\n",
        "        self.is_analog = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.is_analog:\n",
        "            return self.analog_layer(x)\n",
        "        else:\n",
        "            return self.digital_layer(x)\n",
        "\n",
        "    def toggle(self, is_analog):\n",
        "\n",
        "        self.is_analog = is_analog\n",
        "        # If switching to analog, synchronize weights\n",
        "        if self.is_analog:\n",
        "            if self.layer_type == 'conv':\n",
        "                self.analog_layer.set_weights(self.digital_layer.weight.detach().cpu().numpy().reshape(-1))\n",
        "            elif self.layer_type == 'linear':\n",
        "                self.analog_layer.set_weights(self.digital_layer.weight.detach().cpu().numpy(),\n",
        "                                               self.digital_layer.bias.detach().cpu().numpy())\n",
        "\n",
        "        # If switching to digital, synchronize weights\n",
        "        else:\n",
        "            if self.layer_type == 'conv':\n",
        "                weights = torch.tensor(self.analog_layer.get_weights()[0]).reshape(self.digital_layer.weight.shape)\n",
        "                self.digital_layer.weight.data.copy_(weights)\n",
        "            elif self.layer_type == 'linear':\n",
        "                weights, bias = self.analog_layer.get_weights()\n",
        "                self.digital_layer.weight.data.copy_(torch.tensor(weights))\n",
        "                self.digital_layer.bias.data.copy_(torch.tensor(bias))"
      ],
      "metadata": {
        "id": "XDu1mkJOHDyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MTL_Net_DynamicToggle(nn.Module):\n",
        "    def __init__(self, input_channel, num_class):\n",
        "        super(MTL_Net_DynamicToggle, self).__init__()\n",
        "        self.classes = num_class\n",
        "\n",
        "        # Replace layers with dynamic layers\n",
        "        self.conv1 = DynamicLayer('conv', in_channels=input_channel, out_channels=8, kernel_size=3, stride=1)\n",
        "        self.conv2 = DynamicLayer('conv', in_channels=8, out_channels=16, kernel_size=3, stride=1)\n",
        "        self.fc1 = DynamicLayer('linear', 64, 256)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc2 = DynamicLayer('linear', 256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        # Task-specific layers remain digital\n",
        "        self.fc3 = nn.Linear(128, self.classes[0])\n",
        "        self.fc4 = nn.Linear(128, self.classes[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=3)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=3)\n",
        "        x = F.relu(self.fc1(x.reshape(-1, x.shape[1] * x.shape[2] * x.shape[3])))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x1 = self.fc3(x)  # Task 1 output\n",
        "        x2 = self.fc4(x)  # Task 2 output\n",
        "\n",
        "        return x1, x2\n",
        "\n",
        "    def toggle_layer(self, layer_name, is_analog):\n",
        "        \"\"\"\n",
        "        Toggle a specific layer between analog and digital.\n",
        "        \"\"\"\n",
        "        getattr(self, layer_name).toggle(is_analog)"
      ],
      "metadata": {
        "id": "x2RxMQfOHazi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_dynamic(model, trainloader, optim, criterion, epoch, device, rpu_config, toggle_config=None):\n",
        "    \"\"\"\n",
        "    Train function with dynamic analog/digital toggling.\n",
        "\n",
        "    Parameters:\n",
        "    - toggle_config: dict, specifies which layers to toggle and when during training.\n",
        "                     Example: {'conv1': True, 'fc1': False}.\n",
        "    \"\"\"\n",
        "    model.train()  # Ensure the model is in training mode\n",
        "    train_loss, total, total_correct1, total_correct2 = 0, 0, 0, 0\n",
        "\n",
        "    # Apply toggling at the start of training (if toggle_config is provided)\n",
        "    if toggle_config:\n",
        "        for layer_name, is_analog in toggle_config.items():\n",
        "            model.toggle_layer(layer_name, is_analog)\n",
        "\n",
        "    for i, (inputs, tg1, tg2) in enumerate(tqdm(trainloader)):\n",
        "        inputs, tg1, tg2 = inputs.to(device), tg1.to(device), tg2.to(device)\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # Forward pass through the model\n",
        "        op1, op2 = model(inputs)\n",
        "\n",
        "        # Compute losses for both tasks\n",
        "        loss1 = criterion(op1, tg1)\n",
        "        loss2 = criterion(op2, tg2)\n",
        "        total_loss = loss1 + loss2\n",
        "\n",
        "        hardware_loss = compute_hardware_loss(model, op1) + compute_hardware_loss(model, op2)\n",
        "        total_loss += hardware_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optim.step()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        train_loss += total_loss.item()\n",
        "        _, pd1 = torch.max(op1.data, 1)\n",
        "        _, pd2 = torch.max(op2.data, 1)\n",
        "\n",
        "        total_correct1 += (pd1 == tg1).sum().item()\n",
        "        total_correct2 += (pd2 == tg2).sum().item()\n",
        "        total += tg1.size(0)\n",
        "\n",
        "        if total_loss < prev_loss:\n",
        "            update_rpu_config(rpu_config, total_loss*0.000001)\n",
        "        prev_loss = total_loss\n",
        "\n",
        "    print(\"Epoch: [{}]  loss: [{:.2f}] Original_task_acc [{:.2f}] animal_vs_non_animal_acc [{:.2f}]\".format(\n",
        "        epoch + 1, train_loss / (i + 1),\n",
        "        (total_correct1 * 100 / total),\n",
        "        (total_correct2 * 100 / total)\n",
        "    ))\n",
        "\n",
        "    return train_loss / (i + 1), (total_correct1 * 100 / total), (total_correct2 * 100 / total)"
      ],
      "metadata": {
        "id": "mM49uFNsDfVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_dynamic(model, testloader, criterion, epoch, device, toggle_config=None):\n",
        "    \"\"\"\n",
        "    Test function with dynamic analog/digital toggling.\n",
        "\n",
        "    Parameters:\n",
        "    - toggle_config: dict, specifies which layers to toggle during testing.\n",
        "                     Example: {'conv1': True, 'fc1': False}.\n",
        "    \"\"\"\n",
        "    model.eval()  # Ensure the model is in evaluation mode\n",
        "    test_loss, total, total_correct1, total_correct2 = 0, 0, 0, 0\n",
        "\n",
        "    # Apply toggling at the start of testing (if toggle_config is provided)\n",
        "    if toggle_config:\n",
        "        for layer_name, is_analog in toggle_config.items():\n",
        "            model.toggle_layer(layer_name, is_analog)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, tg1, tg2) in enumerate(tqdm(testloader)):\n",
        "            inputs, tg1, tg2 = inputs.to(device), tg1.to(device), tg2.to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            op1, op2 = model(inputs)\n",
        "\n",
        "            # Compute losses for both tasks\n",
        "            loss1 = criterion(op1, tg1)\n",
        "            loss2 = criterion(op2, tg2)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            test_loss += loss1.item() + loss2.item()\n",
        "            _, pd1 = torch.max(op1.data, 1)\n",
        "            _, pd2 = torch.max(op2.data, 1)\n",
        "\n",
        "            total_correct1 += (pd1 == tg1).sum().item()\n",
        "            total_correct2 += (pd2 == tg2).sum().item()\n",
        "            total += tg1.size(0)\n",
        "\n",
        "    # Compute accuracies\n",
        "    acc1 = 100. * total_correct1 / total\n",
        "    acc2 = 100. * total_correct2 / total\n",
        "\n",
        "    # Log metrics\n",
        "    print(\"Test Epoch: [{}]  loss: [{:.2f}] Original_task_Acc [{:.2f}] animal_vs_non_animal_acc [{:.2f}]\".format(\n",
        "        epoch + 1, test_loss / (i + 1), acc1, acc2\n",
        "    ))\n",
        "\n",
        "    return test_loss / (i + 1), acc1, acc2"
      ],
      "metadata": {
        "id": "W69QZdvmHOaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sgd_optimizer(model, learning_rate):\n",
        "    optimizer = AnalogSGD(model.parameters(), lr=learning_rate)\n",
        "    optimizer.regroup_param_groups(model)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "2EMdEGnoJZGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MTL_Net_DynamicToggle(input_channel=3, num_class=[10, 2]).to('cuda')\n",
        "\n",
        "# Start with all analog\n",
        "model.toggle_layer('conv1', is_analog=True)\n",
        "model.toggle_layer('conv2', is_analog=True)\n",
        "model.toggle_layer('fc1', is_analog=True)\n",
        "model.toggle_layer('fc2', is_analog=True)\n",
        "model.toggle_layer('fc3', is_analog=True)\n",
        "model.toggle_layer('fc4', is_analog=True)\n",
        "\n",
        "optimizer = create_sgd_optimizer()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "rpu_config = create_rpu_config()\n",
        "\n",
        "order = ['conv1', 'conv1',('fc1, fc2'),('fc3','fc4')]\n",
        "\n",
        "for epoch in range(50):\n",
        "\n",
        "    # Dynamically toggle during training\n",
        "    train_loss, l1, l2 = train_model_dynamic(model, train_loader, optimizer, criterion, epoch, device='cuda', rpu_config, toggle_config)\n",
        "    if train_loss > 0.8 and order:\n",
        "      val = order.pop()\n",
        "      toggle_config = dict()\n",
        "      if type(val) is tuple:\n",
        "          if val[0] > val[1]:\n",
        "            toggle_config[val[1]] = False\n",
        "            order.append(val[0])\n",
        "          else:\n",
        "            toggle_config[val[0]] = False\n",
        "            order.append(val[1])\n",
        "        else:\n",
        "          toggle_config[val] = False\n",
        "    else:\n",
        "      toggle_config = None\n",
        "\n",
        "    # Dynamically toggle during testing\n",
        "    test_loss, acc1, acc2 = test_model_dynamic(model, test_loader, criterion, epoch, device='cuda', toggle_config)"
      ],
      "metadata": {
        "id": "4NJKWbngIn8h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}